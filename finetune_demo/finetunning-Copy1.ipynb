{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6fb63de2-48c8-47fa-adbe-d6fc0c733249",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-24T11:31:22.953923Z",
     "iopub.status.busy": "2024-03-24T11:31:22.953446Z",
     "iopub.status.idle": "2024-03-24T11:55:35.974946Z",
     "shell.execute_reply": "2024-03-24T11:55:35.974348Z",
     "shell.execute_reply.started": "2024-03-24T11:31:22.953904Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting eos_token is not supported, use the default one.\n",
      "Setting pad_token is not supported, use the default one.\n",
      "Setting unk_token is not supported, use the default one.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34a0e0259f4f46179fc6c1a4fd8622ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1,949,696 || all params: 6,245,533,696 || trainable%: 0.031217444255383614\n",
      "--> Model\n",
      "\n",
      "--> model has 1.949696M params\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting num_proc from 16 back to 1 for the train split to disable multiprocessing as it only contains one shard.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "334fe511010147c5aa8d6a8c7445f9b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting num_proc from 16 back to 1 for the validation split to disable multiprocessing as it only contains one shard.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a500f2ad59f409a9c6a350bc98167e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18278e2141204060957d2f0e6b56510b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=16):   0%|          | 0/41620 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_dataset: Dataset({\n",
      "    features: ['input_ids', 'labels'],\n",
      "    num_rows: 41620\n",
      "})\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29b8e900bbe446a8a8d4e9208862d09e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=16):   0%|          | 0/8324 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/chatglm3-6b-finetunning/lib/python3.10/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n",
      "Detected kernel version 4.19.24, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_dataset: Dataset({\n",
      "    features: ['input_ids', 'output_ids'],\n",
      "    num_rows: 6913\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "max_steps is given, it will override any value given in num_train_epochs\n",
      "***** Running training *****\n",
      "  Num examples = 41,620\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3,500\n",
      "  Number of trainable parameters = 1,949,696\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3500' max='3500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3500/3500 23:44, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge-1</th>\n",
       "      <th>Rouge-2</th>\n",
       "      <th>Rouge-l</th>\n",
       "      <th>Bleu-4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.480100</td>\n",
       "      <td>No log</td>\n",
       "      <td>36.318666</td>\n",
       "      <td>16.808170</td>\n",
       "      <td>30.129816</td>\n",
       "      <td>0.122585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.830900</td>\n",
       "      <td>No log</td>\n",
       "      <td>41.195426</td>\n",
       "      <td>19.661404</td>\n",
       "      <td>34.985790</td>\n",
       "      <td>0.154618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>1.744300</td>\n",
       "      <td>No log</td>\n",
       "      <td>40.598752</td>\n",
       "      <td>19.400780</td>\n",
       "      <td>34.904958</td>\n",
       "      <td>0.153679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>1.453500</td>\n",
       "      <td>No log</td>\n",
       "      <td>39.858026</td>\n",
       "      <td>19.827708</td>\n",
       "      <td>34.493366</td>\n",
       "      <td>0.157400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>2.027100</td>\n",
       "      <td>No log</td>\n",
       "      <td>40.710392</td>\n",
       "      <td>20.005132</td>\n",
       "      <td>34.931954</td>\n",
       "      <td>0.151654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>1.842700</td>\n",
       "      <td>No log</td>\n",
       "      <td>40.683868</td>\n",
       "      <td>19.764560</td>\n",
       "      <td>34.844206</td>\n",
       "      <td>0.157509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>1.894800</td>\n",
       "      <td>No log</td>\n",
       "      <td>40.791334</td>\n",
       "      <td>19.639396</td>\n",
       "      <td>34.507492</td>\n",
       "      <td>0.157120</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.635 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "Saving model checkpoint to ./output/checkpoint-500\n",
      "tokenizer config file saved in ./output/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-500/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./output/checkpoint-1000\n",
      "tokenizer config file saved in ./output/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-1000/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./output/checkpoint-1500\n",
      "tokenizer config file saved in ./output/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-1500/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./output/checkpoint-2000\n",
      "tokenizer config file saved in ./output/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-2000/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./output/checkpoint-2500\n",
      "tokenizer config file saved in ./output/checkpoint-2500/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-2500/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./output/checkpoint-3000\n",
      "tokenizer config file saved in ./output/checkpoint-3000/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-3000/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./output/checkpoint-3500\n",
      "tokenizer config file saved in ./output/checkpoint-3500/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-3500/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%run finetune_hf.py data/ /mnt/workspace/chatglm3-6b/ configs/lora.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "984bb818-015d-4f97-ab9c-aced78fb6f99",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-24T12:11:41.638603Z",
     "iopub.status.busy": "2024-03-24T12:11:41.637995Z",
     "iopub.status.idle": "2024-03-24T12:12:06.465441Z",
     "shell.execute_reply": "2024-03-24T12:12:06.464879Z",
     "shell.execute_reply.started": "2024-03-24T12:11:41.638579Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "baa191a0053d4a3aaf3bb942e891c925",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting eos_token is not supported, use the default one.\n",
      "Setting pad_token is not supported, use the default one.\n",
      "Setting unk_token is not supported, use the default one.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "央行征信局局长王煜出任中国金融培训中心主任\n"
     ]
    }
   ],
   "source": [
    "%run inference_hf.py output/checkpoint-3500/ --prompt \"中国金融培训中心官网显示，原中国人民银行征信中心主任王煜已出任该中心主任。\\n王煜毕业于五道口央行研究生部，在央行体系工作多年，先后出任过央行货币政策司副司长、央行货币政策委员会秘书长，随后王煜进入征信领域，先后出任央行征信管理局局长、中国人民银行征信中心主任。(澎湃)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25ceabe1-2464-4e2e-bf91-b48ee107bcd3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-24T12:23:56.196692Z",
     "iopub.status.busy": "2024-03-24T12:23:56.196095Z",
     "iopub.status.idle": "2024-03-24T13:04:01.062971Z",
     "shell.execute_reply": "2024-03-24T13:04:01.062427Z",
     "shell.execute_reply.started": "2024-03-24T12:23:56.196667Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting eos_token is not supported, use the default one.\n",
      "Setting pad_token is not supported, use the default one.\n",
      "Setting unk_token is not supported, use the default one.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c1e658c41fd444cac532950622fa843",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1,949,696 || all params: 6,245,533,696 || trainable%: 0.031217444255383614\n",
      "--> Model\n",
      "\n",
      "--> model has 1.949696M params\n",
      "\n",
      "train_dataset: Dataset({\n",
      "    features: ['input_ids', 'labels'],\n",
      "    num_rows: 16629\n",
      "})\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e19af824de746cba4b2927bfe00bdd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=16):   0%|          | 0/3326 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/chatglm3-6b-finetunning/lib/python3.10/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n",
      "Detected kernel version 4.19.24, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_dataset: Dataset({\n",
      "    features: ['input_ids', 'output_ids'],\n",
      "    num_rows: 2453\n",
      "})\n",
      "test_dataset: Dataset({\n",
      "    features: ['input_ids', 'output_ids'],\n",
      "    num_rows: 2453\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "max_steps is given, it will override any value given in num_train_epochs\n",
      "Loading model from ./output/checkpoint-2500.\n",
      "***** Running training *****\n",
      "  Num examples = 16,629\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 5,500\n",
      "  Number of trainable parameters = 1,949,696\n",
      "  Continuing training from checkpoint, will skip to saved global_step\n",
      "  Continuing training from epoch 0\n",
      "  Continuing training from global step 2500\n",
      "  Will skip the first 0 epochs then the first 2500 batches in the first epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resume checkpoint from  checkpoint-2500\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5500' max='5500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5500/5500 20:48, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge-1</th>\n",
       "      <th>Rouge-2</th>\n",
       "      <th>Rouge-l</th>\n",
       "      <th>Bleu-4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>1.165900</td>\n",
       "      <td>No log</td>\n",
       "      <td>40.664444</td>\n",
       "      <td>20.709042</td>\n",
       "      <td>36.822720</td>\n",
       "      <td>0.156005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>1.253000</td>\n",
       "      <td>No log</td>\n",
       "      <td>40.709900</td>\n",
       "      <td>19.577864</td>\n",
       "      <td>36.218982</td>\n",
       "      <td>0.157714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>1.440900</td>\n",
       "      <td>No log</td>\n",
       "      <td>40.878140</td>\n",
       "      <td>22.061078</td>\n",
       "      <td>37.898916</td>\n",
       "      <td>0.175130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>1.428200</td>\n",
       "      <td>No log</td>\n",
       "      <td>41.214238</td>\n",
       "      <td>21.136986</td>\n",
       "      <td>37.846252</td>\n",
       "      <td>0.157992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.923400</td>\n",
       "      <td>No log</td>\n",
       "      <td>41.288382</td>\n",
       "      <td>22.014324</td>\n",
       "      <td>38.289306</td>\n",
       "      <td>0.172858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>1.222600</td>\n",
       "      <td>No log</td>\n",
       "      <td>42.650960</td>\n",
       "      <td>22.471748</td>\n",
       "      <td>39.412138</td>\n",
       "      <td>0.178055</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.648 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "Saving model checkpoint to ./output/checkpoint-3000\n",
      "tokenizer config file saved in ./output/checkpoint-3000/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-3000/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./output/checkpoint-3500\n",
      "tokenizer config file saved in ./output/checkpoint-3500/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-3500/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./output/checkpoint-4000\n",
      "tokenizer config file saved in ./output/checkpoint-4000/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-4000/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./output/checkpoint-4500\n",
      "tokenizer config file saved in ./output/checkpoint-4500/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-4500/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./output/checkpoint-5000\n",
      "tokenizer config file saved in ./output/checkpoint-5000/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-5000/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./output/checkpoint-5500\n",
      "tokenizer config file saved in ./output/checkpoint-5500/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-5500/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "***** Running Prediction *****\n",
      "  Num examples = 2453\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run finetune_hf.py data/ /mnt/workspace/chatglm3-6b/ configs/lora.yaml 2500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1ece03b-7540-4461-bef7-8712c3e4c99a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-24T13:10:28.361450Z",
     "iopub.status.busy": "2024-03-24T13:10:28.361013Z",
     "iopub.status.idle": "2024-03-24T13:10:37.940675Z",
     "shell.execute_reply": "2024-03-24T13:10:37.940118Z",
     "shell.execute_reply.started": "2024-03-24T13:10:28.361422Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e0f62b57ae044799655c84999180fc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting eos_token is not supported, use the default one.\n",
      "Setting pad_token is not supported, use the default one.\n",
      "Setting unk_token is not supported, use the default one.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "巴基斯坦卢比兑美元汇率连续跌破150:1\n"
     ]
    }
   ],
   "source": [
    "%run inference_hf.py output/checkpoint-5500/ --prompt \"经济日报-中国经济网5月23日讯 据中国驻卡拉奇总领事馆经济商务室消息，《论坛快报》报道，巴基斯坦央行的数据显示，卢比兑美元汇率近三个工作日内连续下跌，跌幅达5.84%，合8.26卢比。5月20日（本周一）银行间市场卢比兑美元汇率下降1.20%，已接近149.65:1的历史最低点。\\n此外，公开市场卢比兑美元汇率已达153:1。有消息预测称，基于巴目前经济形势，卢比兑美元汇率还将继续下跌，有可能达到165-170:1。\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d8014ca-4726-4e8c-8651-bc56df03d892",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-24T13:17:31.391525Z",
     "iopub.status.busy": "2024-03-24T13:17:31.391200Z",
     "iopub.status.idle": "2024-03-24T13:59:02.242696Z",
     "shell.execute_reply": "2024-03-24T13:59:02.242162Z",
     "shell.execute_reply.started": "2024-03-24T13:17:31.391498Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting eos_token is not supported, use the default one.\n",
      "Setting pad_token is not supported, use the default one.\n",
      "Setting unk_token is not supported, use the default one.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe830e6d398d455bada13a238b1fe3d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1,949,696 || all params: 6,245,533,696 || trainable%: 0.031217444255383614\n",
      "--> Model\n",
      "\n",
      "--> model has 1.949696M params\n",
      "\n",
      "train_dataset: Dataset({\n",
      "    features: ['input_ids', 'labels'],\n",
      "    num_rows: 16629\n",
      "})\n",
      "val_dataset: Dataset({\n",
      "    features: ['input_ids', 'output_ids'],\n",
      "    num_rows: 2453\n",
      "})\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1269bc313554a42860272e0027cdf4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=16):   0%|          | 0/3326 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/chatglm3-6b-finetunning/lib/python3.10/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n",
      "Detected kernel version 4.19.24, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_dataset: Dataset({\n",
      "    features: ['input_ids', 'output_ids'],\n",
      "    num_rows: 2453\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "max_steps is given, it will override any value given in num_train_epochs\n",
      "Loading model from ./output/checkpoint-5500.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resume checkpoint from  checkpoint-5500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 16,629\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 9,000\n",
      "  Number of trainable parameters = 1,949,696\n",
      "  Continuing training from checkpoint, will skip to saved global_step\n",
      "  Continuing training from epoch 0\n",
      "  Continuing training from global step 5500\n",
      "  Will skip the first 0 epochs then the first 5500 batches in the first epoch.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9000' max='9000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [9000/9000 23:23, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge-1</th>\n",
       "      <th>Rouge-2</th>\n",
       "      <th>Rouge-l</th>\n",
       "      <th>Bleu-4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>1.361100</td>\n",
       "      <td>No log</td>\n",
       "      <td>41.599724</td>\n",
       "      <td>22.537166</td>\n",
       "      <td>39.295312</td>\n",
       "      <td>0.181300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>1.424300</td>\n",
       "      <td>No log</td>\n",
       "      <td>40.976968</td>\n",
       "      <td>21.456290</td>\n",
       "      <td>37.993044</td>\n",
       "      <td>0.171269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>1.286100</td>\n",
       "      <td>No log</td>\n",
       "      <td>41.710390</td>\n",
       "      <td>22.203010</td>\n",
       "      <td>38.760150</td>\n",
       "      <td>0.170838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>1.163400</td>\n",
       "      <td>No log</td>\n",
       "      <td>42.709772</td>\n",
       "      <td>22.893328</td>\n",
       "      <td>40.024266</td>\n",
       "      <td>0.178752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>1.428200</td>\n",
       "      <td>No log</td>\n",
       "      <td>43.080532</td>\n",
       "      <td>22.655452</td>\n",
       "      <td>40.232898</td>\n",
       "      <td>0.179721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>1.610700</td>\n",
       "      <td>No log</td>\n",
       "      <td>43.621516</td>\n",
       "      <td>23.977666</td>\n",
       "      <td>40.798076</td>\n",
       "      <td>0.188318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>1.561100</td>\n",
       "      <td>No log</td>\n",
       "      <td>42.844370</td>\n",
       "      <td>23.141972</td>\n",
       "      <td>40.251172</td>\n",
       "      <td>0.175705</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.650 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "Saving model checkpoint to ./output/checkpoint-6000\n",
      "tokenizer config file saved in ./output/checkpoint-6000/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-6000/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./output/checkpoint-6500\n",
      "tokenizer config file saved in ./output/checkpoint-6500/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-6500/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./output/checkpoint-7000\n",
      "tokenizer config file saved in ./output/checkpoint-7000/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-7000/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./output/checkpoint-7500\n",
      "tokenizer config file saved in ./output/checkpoint-7500/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-7500/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./output/checkpoint-8000\n",
      "tokenizer config file saved in ./output/checkpoint-8000/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-8000/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./output/checkpoint-8500\n",
      "tokenizer config file saved in ./output/checkpoint-8500/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-8500/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./output/checkpoint-9000\n",
      "tokenizer config file saved in ./output/checkpoint-9000/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-9000/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "***** Running Prediction *****\n",
      "  Num examples = 2453\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run finetune_hf.py data/ /mnt/workspace/chatglm3-6b/ configs/lora.yaml yes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2cfc4737-4204-4189-9ddb-e21deaff7dc9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-25T04:14:35.734534Z",
     "iopub.status.busy": "2024-03-25T04:14:35.734032Z",
     "iopub.status.idle": "2024-03-25T04:53:20.734739Z",
     "shell.execute_reply": "2024-03-25T04:53:20.734212Z",
     "shell.execute_reply.started": "2024-03-25T04:14:35.734514Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting eos_token is not supported, use the default one.\n",
      "Setting pad_token is not supported, use the default one.\n",
      "Setting unk_token is not supported, use the default one.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42c6038b06b24a33a00cf6e8c22aeb84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1,949,696 || all params: 6,245,533,696 || trainable%: 0.031217444255383614\n",
      "--> Model\n",
      "\n",
      "--> model has 1.949696M params\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/chatglm3-6b-finetunning/lib/python3.10/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n",
      "Detected kernel version 4.19.24, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_dataset: Dataset({\n",
      "    features: ['input_ids', 'labels'],\n",
      "    num_rows: 16629\n",
      "})\n",
      "val_dataset: Dataset({\n",
      "    features: ['input_ids', 'output_ids'],\n",
      "    num_rows: 2453\n",
      "})\n",
      "test_dataset: Dataset({\n",
      "    features: ['input_ids', 'output_ids'],\n",
      "    num_rows: 2453\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "max_steps is given, it will override any value given in num_train_epochs\n",
      "Loading model from ./output/checkpoint-9000.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resume checkpoint from  checkpoint-9000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 16,629\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 12,000\n",
      "  Number of trainable parameters = 1,949,696\n",
      "  Continuing training from checkpoint, will skip to saved global_step\n",
      "  Continuing training from epoch 0\n",
      "  Continuing training from global step 9000\n",
      "  Will skip the first 0 epochs then the first 9000 batches in the first epoch.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='12000' max='12000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [12000/12000 20:22, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge-1</th>\n",
       "      <th>Rouge-2</th>\n",
       "      <th>Rouge-l</th>\n",
       "      <th>Bleu-4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>1.837500</td>\n",
       "      <td>No log</td>\n",
       "      <td>43.208518</td>\n",
       "      <td>22.754758</td>\n",
       "      <td>39.479712</td>\n",
       "      <td>0.178931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>1.312000</td>\n",
       "      <td>No log</td>\n",
       "      <td>43.842766</td>\n",
       "      <td>22.694966</td>\n",
       "      <td>40.859810</td>\n",
       "      <td>0.177772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>1.600700</td>\n",
       "      <td>No log</td>\n",
       "      <td>44.015074</td>\n",
       "      <td>23.303598</td>\n",
       "      <td>41.303696</td>\n",
       "      <td>0.182090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>1.174500</td>\n",
       "      <td>No log</td>\n",
       "      <td>43.080146</td>\n",
       "      <td>22.983194</td>\n",
       "      <td>40.342544</td>\n",
       "      <td>0.183069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>1.392600</td>\n",
       "      <td>No log</td>\n",
       "      <td>43.297870</td>\n",
       "      <td>22.799620</td>\n",
       "      <td>40.585516</td>\n",
       "      <td>0.178429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>1.546700</td>\n",
       "      <td>No log</td>\n",
       "      <td>43.529448</td>\n",
       "      <td>22.720256</td>\n",
       "      <td>40.623170</td>\n",
       "      <td>0.178171</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache /tmp/jieba.cache\n",
      "Loading model cost 0.483 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "Saving model checkpoint to ./output/checkpoint-9500\n",
      "tokenizer config file saved in ./output/checkpoint-9500/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-9500/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./output/checkpoint-10000\n",
      "tokenizer config file saved in ./output/checkpoint-10000/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-10000/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./output/checkpoint-10500\n",
      "tokenizer config file saved in ./output/checkpoint-10500/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-10500/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./output/checkpoint-11000\n",
      "tokenizer config file saved in ./output/checkpoint-11000/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-11000/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./output/checkpoint-11500\n",
      "tokenizer config file saved in ./output/checkpoint-11500/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-11500/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./output/checkpoint-12000\n",
      "tokenizer config file saved in ./output/checkpoint-12000/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-12000/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "***** Running Prediction *****\n",
      "  Num examples = 2453\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run finetune_hf.py data/ /mnt/workspace/chatglm3-6b/ configs/lora.yaml yes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7216aea2-1fb8-405f-872e-ebf18a3360b5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-25T04:54:46.033446Z",
     "iopub.status.busy": "2024-03-25T04:54:46.033117Z",
     "iopub.status.idle": "2024-03-25T05:34:32.238063Z",
     "shell.execute_reply": "2024-03-25T05:34:32.237483Z",
     "shell.execute_reply.started": "2024-03-25T04:54:46.033423Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting eos_token is not supported, use the default one.\n",
      "Setting pad_token is not supported, use the default one.\n",
      "Setting unk_token is not supported, use the default one.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b01f7b4bd89b47e7a96b235edcf5d2a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1,949,696 || all params: 6,245,533,696 || trainable%: 0.031217444255383614\n",
      "--> Model\n",
      "\n",
      "--> model has 1.949696M params\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/chatglm3-6b-finetunning/lib/python3.10/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n",
      "Detected kernel version 4.19.24, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_dataset: Dataset({\n",
      "    features: ['input_ids', 'labels'],\n",
      "    num_rows: 16629\n",
      "})\n",
      "val_dataset: Dataset({\n",
      "    features: ['input_ids', 'output_ids'],\n",
      "    num_rows: 2453\n",
      "})\n",
      "test_dataset: Dataset({\n",
      "    features: ['input_ids', 'output_ids'],\n",
      "    num_rows: 2453\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "max_steps is given, it will override any value given in num_train_epochs\n",
      "Loading model from ./output/checkpoint-12000.\n",
      "***** Running training *****\n",
      "  Num examples = 16,629\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 15,000\n",
      "  Number of trainable parameters = 1,949,696\n",
      "  Continuing training from checkpoint, will skip to saved global_step\n",
      "  Continuing training from epoch 0\n",
      "  Continuing training from global step 12000\n",
      "  Will skip the first 0 epochs then the first 12000 batches in the first epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resume checkpoint from  checkpoint-12000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15000' max='15000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15000/15000 21:45, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge-1</th>\n",
       "      <th>Rouge-2</th>\n",
       "      <th>Rouge-l</th>\n",
       "      <th>Bleu-4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>1.511300</td>\n",
       "      <td>No log</td>\n",
       "      <td>43.887954</td>\n",
       "      <td>23.071326</td>\n",
       "      <td>40.125942</td>\n",
       "      <td>0.179782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>1.487000</td>\n",
       "      <td>No log</td>\n",
       "      <td>44.697092</td>\n",
       "      <td>24.245912</td>\n",
       "      <td>41.776092</td>\n",
       "      <td>0.198397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>0.990300</td>\n",
       "      <td>No log</td>\n",
       "      <td>42.623466</td>\n",
       "      <td>21.780008</td>\n",
       "      <td>39.249898</td>\n",
       "      <td>0.174142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>1.814400</td>\n",
       "      <td>No log</td>\n",
       "      <td>43.940204</td>\n",
       "      <td>23.301348</td>\n",
       "      <td>40.386672</td>\n",
       "      <td>0.185679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14500</td>\n",
       "      <td>1.006700</td>\n",
       "      <td>No log</td>\n",
       "      <td>43.523992</td>\n",
       "      <td>22.653408</td>\n",
       "      <td>39.696516</td>\n",
       "      <td>0.181841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>1.639700</td>\n",
       "      <td>No log</td>\n",
       "      <td>43.477294</td>\n",
       "      <td>22.951938</td>\n",
       "      <td>39.670938</td>\n",
       "      <td>0.184039</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.562 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "Saving model checkpoint to ./output/checkpoint-12500\n",
      "tokenizer config file saved in ./output/checkpoint-12500/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-12500/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./output/checkpoint-13000\n",
      "tokenizer config file saved in ./output/checkpoint-13000/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-13000/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./output/checkpoint-13500\n",
      "tokenizer config file saved in ./output/checkpoint-13500/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-13500/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./output/checkpoint-14000\n",
      "tokenizer config file saved in ./output/checkpoint-14000/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-14000/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./output/checkpoint-14500\n",
      "tokenizer config file saved in ./output/checkpoint-14500/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-14500/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./output/checkpoint-15000\n",
      "tokenizer config file saved in ./output/checkpoint-15000/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-15000/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "***** Running Prediction *****\n",
      "  Num examples = 2453\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation metrics: {'rouge-1': 45.59507452099469, 'rouge-2': 26.605518181818184, 'rouge-l': 42.35416428862617, 'bleu-4': 0.1998162016460831}\n"
     ]
    }
   ],
   "source": [
    "%run finetune_hf.py data/ /mnt/workspace/chatglm3-6b/ configs/lora.yaml yes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatglm3-6b-finetunning",
   "language": "python",
   "name": "chatglm3-6b-finetunning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
